[
    {
        "functionQualifiedName": "org.apache.hadoop.fs.viewfs.ViewFs.getHomeDirectory",
        "serviceName": "hadoop-common-project/hadoop-common",
        "noOfNonPrimitiveParameters": 0,
        "functionStr": "public Path getHomeDirectory() {\n    if (homeDir == null) {\n      String base = fsState.getHomeDirPrefixValue();\n      if (base == null) {\n        base = \"/user\";\n      }\n      homeDir = (base.equals(\"/\") ?\n        this.makeQualified(new Path(base + ugi.getShortUserName())):\n        this.makeQualified(new Path(base + \"/\" + ugi.getShortUserName())));\n    }\n    return homeDir;\n  }",
        "difficultyLevel": "easy"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.getFileLinkStatus",
        "serviceName": "hadoop-common-project/hadoop-common",
        "noOfNonPrimitiveParameters": 1,
        "functionStr": "public FileStatus getFileLinkStatus(final Path f)\n     throws AccessControlException, FileNotFoundException,\n     UnsupportedFileSystemException, IOException {\n    InodeTree.ResolveResult<AbstractFileSystem> res =\n      fsState.resolve(getUriPath(f), false); // do not follow mount link\n    return res.targetFileSystem.getFileLinkStatus(res.remainingPath);\n  }",
        "difficultyLevel": "medium"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.mkdir",
        "serviceName": "hadoop-common-project/hadoop-common",
        "noOfNonPrimitiveParameters": 2,
        "functionStr": "public void mkdir(final Path dir, final FsPermission permission,\n        final boolean createParent) throws IOException {\n      if (theInternalDir.isRoot() && dir == null) {\n        throw new FileAlreadyExistsException(\"/ already exits\");\n      }\n\n      if (this.fsState.getRootFallbackLink() != null) {\n        AbstractFileSystem linkedFallbackFs =\n            this.fsState.getRootFallbackLink().getTargetFileSystem();\n        Path parent = Path.getPathWithoutSchemeAndAuthority(\n            new Path(theInternalDir.fullPath));\n        String leafChild = (InodeTree.SlashPath.equals(dir)) ?\n            InodeTree.SlashPath.toString() :\n            dir.getName();\n        Path dirToCreate = new Path(parent, leafChild);\n        try {\n          // We are here because, the parent dir already exist in the mount\n          // table internal tree. So, let's create parent always in fallback.\n          linkedFallbackFs.mkdir(dirToCreate, permission, true);\n          return;\n        } catch (IOException e) {\n          if (LOG.isDebugEnabled()) {\n            StringBuilder msg = new StringBuilder(\"Failed to create {}\")\n                .append(\" at fallback fs : {}\");\n            LOG.debug(msg.toString(), dirToCreate, linkedFallbackFs.getUri());\n          }\n          throw e;\n        }\n      }\n\n      throw readOnlyMountTable(\"mkdir\", dir);\n    }",
        "difficultyLevel": "difficult"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.fs.s3a.impl.DeleteOperation.queueForDeletion",
        "serviceName": "hadoop-tools/hadoop-aws",
        "noOfNonPrimitiveParameters": 0,
        "functionStr": "private void queueForDeletion(final String key,\n      boolean isDirMarker) throws IOException {\n    LOG.debug(\"Adding object to delete: \\\"{}\\\"\", key);\n    keys.add(new DeleteEntry(key, isDirMarker));\n    if (keys.size() == pageSize) {\n      submitNextBatch();\n    }\n  }",
        "difficultyLevel": "easy"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.fs.s3a.impl.CreateFileBuilder.withFlags",
        "serviceName": "hadoop-tools/hadoop-aws",
        "noOfNonPrimitiveParameters": 1,
        "functionStr": "public CreateFileBuilder withFlags(EnumSet<CreateFlag> flags) {\n    if (flags.contains(CreateFlag.CREATE)) {\n      create();\n    }\n    if (flags.contains(CreateFlag.APPEND)) {\n      append();\n    }\n    overwrite(flags.contains(CreateFlag.OVERWRITE));\n    return this;\n  }",
        "difficultyLevel": "medium"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.fs.s3a.impl.DirMarkerTracker.removeParentMarkers",
        "serviceName": "hadoop-tools/hadoop-aws",
        "noOfNonPrimitiveParameters": 2,
        "functionStr": "private void removeParentMarkers(final Path path,\n      List<Marker> removed) {\n    if (path == null || path.isRoot()) {\n      return;\n    }\n    scanCount++;\n    removeParentMarkers(path.getParent(), removed);\n    final Marker value = leafMarkers.remove(path);\n    if (value != null) {\n      // marker is surplus\n      removed.add(value);\n      if (recordSurplusMarkers) {\n        surplusMarkers.put(path, value);\n      }\n    }\n  }",
        "difficultyLevel": "difficult"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.maven.plugin.util.FileSetUtils.getCommaSeparatedList",
        "serviceName": "hadoop-maven-plugins",
        "noOfNonPrimitiveParameters": 1,
        "functionStr": "private static String getCommaSeparatedList(List<String> list) {\n    StringBuilder buffer = new StringBuilder();\n    String separator = \"\";\n    for (Object e : list) {\n      buffer.append(separator).append(e);\n      separator = \",\";\n    }\n    return buffer.toString();\n  }",
        "difficultyLevel": "easy"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.maven.plugin.util.Exec.envToString",
        "serviceName": "hadoop-maven-plugins",
        "noOfNonPrimitiveParameters": 1,
        "functionStr": "public static String envToString(Map<String, String> env) {\n    StringBuilder bld = new StringBuilder();\n    bld.append(\"{\");\n    if (env != null) {\n      for (Map.Entry<String, String> entry : env.entrySet()) {\n        String val = entry.getValue();\n        if (val == null) {\n          val = \"\";\n        }\n        bld.append(\"\\n  \").append(entry.getKey()).\n              append(\" = '\").append(val).append(\"'\\n\");\n      }\n    }\n    bld.append(\"}\");\n    return bld.toString();\n  }",
        "difficultyLevel": "medium"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.maven.plugin.util.Exec.run",
        "serviceName": "hadoop-maven-plugins",
        "noOfNonPrimitiveParameters": 3,
        "functionStr": "public int run(List<String> command, List<String> output,\n      List<String> errors) {\n    int retCode = 1;\n    ProcessBuilder pb = new ProcessBuilder(command);\n    try {\n      Process p = pb.start();\n      OutputBufferThread stdOut = new OutputBufferThread(p.getInputStream());\n      OutputBufferThread stdErr = new OutputBufferThread(p.getErrorStream());\n      stdOut.start();\n      stdErr.start();\n      retCode = p.waitFor();\n      if (retCode != 0) {\n        mojo.getLog().warn(command + \" failed with error code \" + retCode);\n        for (String s : stdErr.getOutput()) {\n          mojo.getLog().debug(s);\n        }\n      }\n      stdOut.join();\n      stdErr.join();\n      output.addAll(stdOut.getOutput());\n      if (errors != null) {\n        errors.addAll(stdErr.getOutput());\n      }\n    } catch (IOException ioe) {\n      mojo.getLog().warn(command + \" failed: \" + ioe.toString());\n    } catch (InterruptedException ie) {\n      mojo.getLog().warn(command + \" failed: \" + ie.toString());\n    }\n    return retCode;\n  }",
        "difficultyLevel": "difficult"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.hdfs.DFSUtil.isValidNameForComponent",
        "serviceName": "hadoop-hdfs-project/hadoop-hdfs",
        "noOfNonPrimitiveParameters": 0,
        "functionStr": "public static boolean isValidNameForComponent(String component) {\n    if (component.equals(\".\") ||\n        component.equals(\"..\") ||\n        component.indexOf(\":\") >= 0 ||\n        component.indexOf(\"/\") >= 0) {\n      return false;\n    }\n    return !isReservedPathComponent(component);\n  }",
        "difficultyLevel": "easy"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.hdfs.HAUtil.getNameNodeId",
        "serviceName": "hadoop-hdfs-project/hadoop-hdfs",
        "noOfNonPrimitiveParameters": 1,
        "functionStr": "public static String getNameNodeId(Configuration conf, String nsId) {\n    String namenodeId = conf.getTrimmed(DFS_HA_NAMENODE_ID_KEY);\n    if (namenodeId != null) {\n      return namenodeId;\n    }\n    \n    String suffixes[] = DFSUtil.getSuffixIDs(conf, DFS_NAMENODE_RPC_ADDRESS_KEY,\n        nsId, null, DFSUtil.LOCAL_ADDRESS_MATCHER);\n    if (suffixes == null) {\n      String msg = \"Configuration \" + DFS_NAMENODE_RPC_ADDRESS_KEY + \n          \" must be suffixed with nameservice and namenode ID for HA \" +\n          \"configuration.\";\n      throw new HadoopIllegalArgumentException(msg);\n    }\n    \n    return suffixes[1];\n  }",
        "difficultyLevel": "medium"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.hdfs.HAUtil.isHAEnabled",
        "serviceName": "hadoop-hdfs-project/hadoop-hdfs",
        "noOfNonPrimitiveParameters": 2,
        "functionStr": "public static boolean isHAEnabled(Configuration conf, String nsId) {\n    Map<String, Map<String, InetSocketAddress>> addresses =\n        DFSUtilClient.getHaNnRpcAddresses(conf);\n    if (addresses == null) return false;\n    Map<String, InetSocketAddress> nnMap = addresses.get(nsId);\n    return nnMap != null && nnMap.size() > 1;\n  }",
        "difficultyLevel": "difficult"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Metrics.addWrite",
        "serviceName": "hadoop-hdfs-project/hadoop-hdfs-nfs",
        "noOfNonPrimitiveParameters": 0,
        "functionStr": "public void addWrite(long latencyNanos) {\n    write.add(latencyNanos);\n    for (MutableQuantiles q : writeNanosQuantiles) {\n      q.add(latencyNanos);\n    }\n  }",
        "difficultyLevel": "easy"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Metrics.create",
        "serviceName": "hadoop-hdfs-project/hadoop-hdfs-nfs",
        "noOfNonPrimitiveParameters": 1,
        "functionStr": "public static Nfs3Metrics create(Configuration conf, String gatewayName) {\n    String sessionId = conf.get(DFSConfigKeys.DFS_METRICS_SESSION_ID_KEY);\n    MetricsSystem ms = DefaultMetricsSystem.instance();\n    JvmMetrics jm = JvmMetrics.create(gatewayName, sessionId, ms);\n\n    // Percentile measurement is [50th,75th,90th,95th,99th] currently \n    int[] intervals = conf\n        .getInts(NfsConfigKeys.NFS_METRICS_PERCENTILES_INTERVALS_KEY);\n    return ms.register(new Nfs3Metrics(gatewayName, sessionId, intervals, jm));\n  }",
        "difficultyLevel": "medium"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Utils.writeChannel",
        "serviceName": "hadoop-hdfs-project/hadoop-hdfs-nfs",
        "noOfNonPrimitiveParameters": 2,
        "functionStr": "public static void writeChannel(Channel channel, XDR out, int xid) {\n    if (channel == null) {\n      RpcProgramNfs3.LOG\n          .info(\"Null channel should only happen in tests. Do nothing.\");\n      return;\n    }\n    \n    if (RpcProgramNfs3.LOG.isDebugEnabled()) {\n      RpcProgramNfs3.LOG.debug(WRITE_RPC_END + xid);\n    }\n    ByteBuf outBuf = XDR.writeMessageTcp(out, true);\n    channel.writeAndFlush(outBuf);\n  }",
        "difficultyLevel": "difficult"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token.toString",
        "serviceName": "hadoop-common-project/hadoop-auth",
        "noOfNonPrimitiveParameters": 0,
        "functionStr": "public String toString() {\n      String value = \"\";\n      HttpCookie authCookie = cookieHandler.getAuthCookie();\n      if (authCookie != null) {\n        value = authCookie.getValue();\n        if (value.startsWith(\"\\\"\")) { // tests don't want the quotes.\n          value = value.substring(1, value.length()-1);\n        }\n      }\n      return value;\n    }",
        "difficultyLevel": "easy"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.security.authentication.client.AuthenticatedURL$AuthCookieHandler.setAuthCookie",
        "serviceName": "hadoop-common-project/hadoop-auth",
        "noOfNonPrimitiveParameters": 1,
        "functionStr": "private synchronized void setAuthCookie(HttpCookie cookie) {\n      final HttpCookie oldCookie = authCookie;\n      // will redefine if new cookie is valid.\n      authCookie = null;\n      cookieHeaders = Collections.emptyMap();\n      boolean valid = cookie != null && !cookie.getValue().isEmpty() &&\n          !cookie.hasExpired();\n      if (valid) {\n        // decrease lifetime to avoid using a cookie soon to expire.\n        // allows authenticators to pre-emptively reauthenticate to\n        // prevent clients unnecessarily receiving a 401.\n        long maxAge = cookie.getMaxAge();\n        if (maxAge != -1) {\n          cookie.setMaxAge(maxAge * 9/10);\n          valid = !cookie.hasExpired();\n        }\n      }\n      if (valid) {\n        // v0 cookies value aren't quoted by default but tomcat demands\n        // quoting.\n        if (cookie.getVersion() == 0) {\n          String value = cookie.getValue();\n          if (!value.startsWith(\"\\\"\")) {\n            value = \"\\\"\" + value + \"\\\"\";\n            cookie.setValue(value);\n          }\n        }\n        authCookie = cookie;\n        cookieHeaders = new HashMap<>();\n        cookieHeaders.put(\"Cookie\", Arrays.asList(cookie.toString()));\n      }\n    }",
        "difficultyLevel": "medium"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection",
        "serviceName": "hadoop-common-project/hadoop-auth",
        "noOfNonPrimitiveParameters": 2,
        "functionStr": "public HttpURLConnection openConnection(URL url, Token token) throws IOException, AuthenticationException {\n    if (url == null) {\n      throw new IllegalArgumentException(\"url cannot be NULL\");\n    }\n    if (!url.getProtocol().equalsIgnoreCase(\"http\") && !url.getProtocol().equalsIgnoreCase(\"https\")) {\n      throw new IllegalArgumentException(\"url must be for a HTTP or HTTPS resource\");\n    }\n    if (token == null) {\n      throw new IllegalArgumentException(\"token cannot be NULL\");\n    }\n    authenticator.authenticate(url, token);\n\n    // allow the token to create the connection with a cookie handler for\n    // managing session cookies.\n    return token.openConnection(url, connConfigurator);\n  }",
        "difficultyLevel": "difficult"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.registry.client.binding.RegistryTypeUtils.getAddressField",
        "serviceName": "hadoop-common-project/hadoop-registry",
        "noOfNonPrimitiveParameters": 0,
        "functionStr": "public static String getAddressField(Map<String, String> address,\n      String field) throws InvalidRecordException {\n    String val = address.get(field);\n    if (val == null) {\n      throw new InvalidRecordException(\"\", \"Missing address field: \" + field);\n    }\n    return val;\n  }",
        "difficultyLevel": "easy"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.registry.client.binding.RegistryTypeUtils.retrieveAddressesUriType",
        "serviceName": "hadoop-common-project/hadoop-registry",
        "noOfNonPrimitiveParameters": 1,
        "functionStr": "public static List<String> retrieveAddressesUriType(Endpoint epr)\n      throws InvalidRecordException {\n    if (epr == null) {\n      return null;\n    }\n    requireAddressType(ADDRESS_URI, epr);\n    List<Map<String, String>> addresses = epr.addresses;\n    if (addresses.size() < 1) {\n      throw new InvalidRecordException(epr.toString(),\n          \"No addresses in endpoint\");\n    }\n    List<String> results = new ArrayList<String>(addresses.size());\n    for (Map<String, String> address : addresses) {\n      results.add(getAddressField(address, ADDRESS_URI));\n    }\n    return results;\n  }",
        "difficultyLevel": "medium"
    },
    {
        "functionQualifiedName": "org.apache.hadoop.registry.client.binding.RegistryUtils.statChildren",
        "serviceName": "hadoop-common-project/hadoop-registry",
        "noOfNonPrimitiveParameters": 1,
        "functionStr": "public static Map<String, RegistryPathStatus> statChildren(\n      RegistryOperations registryOperations,\n      String path)\n      throws PathNotFoundException,\n      InvalidPathnameException,\n      IOException {\n    List<String> childNames = registryOperations.list(path);\n    Map<String, RegistryPathStatus> results =\n        new HashMap<String, RegistryPathStatus>();\n    for (String childName : childNames) {\n      String child = join(path, childName);\n      try {\n        RegistryPathStatus stat = registryOperations.stat(child);\n        results.put(childName, stat);\n      } catch (PathNotFoundException pnfe) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"stat failed on {}: moved? {}\", child, pnfe, pnfe);\n        }\n        // and continue\n      }\n    }\n    return results;\n  }",
        "difficultyLevel": "difficult"
    }
]
