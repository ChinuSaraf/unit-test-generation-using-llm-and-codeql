[
  {
      "functionId": 1,
      "functionQualifiedName": "org.apache.hadoop.fs.viewfs.ViewFs.getHomeDirectory",
      "serviceName": "hadoop-common-project/hadoop-common",
      "functionStr": "public Path getHomeDirectory() {\n    if (homeDir == null) {\n      String base = fsState.getHomeDirPrefixValue();\n      if (base == null) {\n        base = \"/user\";\n      }\n      homeDir = (base.equals(\"/\") ?\n        this.makeQualified(new Path(base + ugi.getShortUserName())):\n        this.makeQualified(new Path(base + \"/\" + ugi.getShortUserName())));\n    }\n    return homeDir;\n  }",
      "noOfNonPrimitiveParameters": 0,
      "noOfLinesOfCode": 12,
      "difficultyLevel": "easy"
  },
  {
      "functionId": 2,
      "functionQualifiedName": "org.apache.hadoop.fs.viewfs.ViewFs.getFileLinkStatus",
      "serviceName": "hadoop-common-project/hadoop-common",
      "functionStr": "public FileStatus getFileLinkStatus(final Path f)\n     throws AccessControlException, FileNotFoundException,\n     UnsupportedFileSystemException, IOException {\n    InodeTree.ResolveResult<AbstractFileSystem> res =\n      fsState.resolve(getUriPath(f), false); // do not follow mount link\n    return res.targetFileSystem.getFileLinkStatus(res.remainingPath);\n  }",
      "noOfNonPrimitiveParameters": 1,
      "noOfLinesOfCode": 7,
      "difficultyLevel": "medium"
  },
  {
      "functionId": 3,
      "functionQualifiedName": "org.apache.hadoop.fs.viewfs.ViewFs.mkdir",
      "serviceName": "hadoop-common-project/hadoop-common",
      "functionStr": "public void mkdir(final Path dir, final FsPermission permission,\n        final boolean createParent) throws IOException {\n      if (theInternalDir.isRoot() && dir == null) {\n        throw new FileAlreadyExistsException(\"/ already exits\");\n      }\n\n      if (this.fsState.getRootFallbackLink() != null) {\n        AbstractFileSystem linkedFallbackFs =\n            this.fsState.getRootFallbackLink().getTargetFileSystem();\n        Path parent = Path.getPathWithoutSchemeAndAuthority(\n            new Path(theInternalDir.fullPath));\n        String leafChild = (InodeTree.SlashPath.equals(dir)) ?\n            InodeTree.SlashPath.toString() :\n            dir.getName();\n        Path dirToCreate = new Path(parent, leafChild);\n        try {\n          // We are here because, the parent dir already exist in the mount\n          // table internal tree. So, let's create parent always in fallback.\n          linkedFallbackFs.mkdir(dirToCreate, permission, true);\n          return;\n        } catch (IOException e) {\n          if (LOG.isDebugEnabled()) {\n            StringBuilder msg = new StringBuilder(\"Failed to create {}\")\n                .append(\" at fallback fs : {}\");\n            LOG.debug(msg.toString(), dirToCreate, linkedFallbackFs.getUri());\n          }\n          throw e;\n        }\n      }\n\n      throw readOnlyMountTable(\"mkdir\", dir);\n    }",
      "noOfNonPrimitiveParameters": 2,
      "noOfLinesOfCode": 8,
      "difficultyLevel": "difficult"
  },
  {
      "functionId": 4,
      "functionQualifiedName": "org.apache.hadoop.fs.s3a.impl.ChangeTracker.toString",
      "serviceName": "hadoop-tools/hadoop-aws",
      "functionStr": "public String toString() {\n    final StringBuilder sb = new StringBuilder(\n        \"ChangeTracker{\");\n    sb.append(policy);\n    sb.append(\", revisionId='\").append(revisionId).append('\\'');\n    sb.append('}');\n    return sb.toString();\n  }",
      "noOfNonPrimitiveParameters": 0,
      "noOfLinesOfCode": 8,
      "difficultyLevel": "easy"
  },
  {
      "functionId": 5,
      "functionQualifiedName": "org.apache.hadoop.fs.s3a.impl.CreateFileBuilder.withFlags",
      "serviceName": "hadoop-tools/hadoop-aws",
      "functionStr": "public CreateFileBuilder withFlags(EnumSet<CreateFlag> flags) {\n    if (flags.contains(CreateFlag.CREATE)) {\n      create();\n    }\n    if (flags.contains(CreateFlag.APPEND)) {\n      append();\n    }\n    overwrite(flags.contains(CreateFlag.OVERWRITE));\n    return this;\n  }",
      "noOfNonPrimitiveParameters": 1,
      "noOfLinesOfCode": 9,
      "difficultyLevel": "medium"
  },
  {
      "functionId": 6,
      "functionQualifiedName": "org.apache.hadoop.fs.s3a.impl.DirMarkerTracker.removeParentMarkers",
      "serviceName": "hadoop-tools/hadoop-aws",
      "functionStr": "private void removeParentMarkers(final Path path,\n      List<Marker> removed) {\n    if (path == null || path.isRoot()) {\n      return;\n    }\n    scanCount++;\n    removeParentMarkers(path.getParent(), removed);\n    final Marker value = leafMarkers.remove(path);\n    if (value != null) {\n      // marker is surplus\n      removed.add(value);\n      if (recordSurplusMarkers) {\n        surplusMarkers.put(path, value);\n      }\n    }\n  }",
      "noOfNonPrimitiveParameters": 2,
      "noOfLinesOfCode": 15,
      "difficultyLevel": "difficult"
  },
  {
      "functionId": 7,
      "functionQualifiedName": "org.apache.hadoop.maven.plugin.util.FileSetUtils.getCommaSeparatedList",
      "serviceName": "hadoop-maven-plugins",
      "functionStr": "private static String getCommaSeparatedList(List<String> list) {\n    StringBuilder buffer = new StringBuilder();\n    String separator = \"\";\n    for (Object e : list) {\n      buffer.append(separator).append(e);\n      separator = \",\";\n    }\n    return buffer.toString();\n  }",
      "noOfNonPrimitiveParameters": 1,
      "noOfLinesOfCode": 9,
      "difficultyLevel": "easy"
  },
  {
      "functionId": 8,
      "functionQualifiedName": "org.apache.hadoop.maven.plugin.util.Exec.envToString",
      "serviceName": "hadoop-maven-plugins",
      "functionStr": "public static String envToString(Map<String, String> env) {\n    StringBuilder bld = new StringBuilder();\n    bld.append(\"{\");\n    if (env != null) {\n      for (Map.Entry<String, String> entry : env.entrySet()) {\n        String val = entry.getValue();\n        if (val == null) {\n          val = \"\";\n        }\n        bld.append(\"\\n  \").append(entry.getKey()).\n              append(\" = '\").append(val).append(\"'\\n\");\n      }\n    }\n    bld.append(\"}\");\n    return bld.toString();\n  }",
      "noOfNonPrimitiveParameters": 1,
      "noOfLinesOfCode": 17,
      "difficultyLevel": "medium"
  },
  {
      "functionId": 9,
      "functionQualifiedName": "org.apache.hadoop.maven.plugin.cmakebuilder.CompileMojo.validateSourceParams",
      "serviceName": "hadoop-maven-plugins",
      "functionStr": "static void validateSourceParams(File source, File output)\n      throws MojoExecutionException {\n    String cOutput = null, cSource = null;\n    try {\n      cOutput = output.getCanonicalPath();\n    } catch (IOException e) {\n      throw new MojoExecutionException(\"error getting canonical path \" +\n          \"for output\", e);\n    }\n    try {\n      cSource = source.getCanonicalPath();\n    } catch (IOException e) {\n      throw new MojoExecutionException(\"error getting canonical path \" +\n          \"for source\", e);\n    }\n\n    // This doesn't catch all the bad cases-- we could be following symlinks or\n    // hardlinks, etc.  However, this will usually catch a common mistake.\n    if (cSource.startsWith(cOutput)) {\n      throw new MojoExecutionException(\"The source directory must not be \" +\n          \"inside the output directory (it would be destroyed by \" +\n          \"'mvn clean')\");\n    }\n  }",
      "noOfNonPrimitiveParameters": 2,
      "noOfLinesOfCode": 24,
      "difficultyLevel": "difficult"
  },
  {
      "functionId": 10,
      "functionQualifiedName": "org.apache.hadoop.hdfs.DFSUtil.isValidNameForComponent",
      "serviceName": "hadoop-hdfs-project/hadoop-hdfs",
      "functionStr": "public static boolean isValidNameForComponent(String component) {\n    if (component.equals(\".\") ||\n        component.equals(\"..\") ||\n        component.indexOf(\":\") >= 0 ||\n        component.indexOf(\"/\") >= 0) {\n      return false;\n    }\n    return !isReservedPathComponent(component);\n  }",
      "noOfNonPrimitiveParameters": 0,
      "noOfLinesOfCode": 9,
      "difficultyLevel": "easy"
  },
  {
      "functionId": 11,
      "functionQualifiedName": "org.apache.hadoop.hdfs.HAUtil.getNameNodeId",
      "serviceName": "hadoop-hdfs-project/hadoop-hdfs",
      "functionStr": "public static String getNameNodeId(Configuration conf, String nsId) {\n    String namenodeId = conf.getTrimmed(DFS_HA_NAMENODE_ID_KEY);\n    if (namenodeId != null) {\n      return namenodeId;\n    }\n    \n    String suffixes[] = DFSUtil.getSuffixIDs(conf, DFS_NAMENODE_RPC_ADDRESS_KEY,\n        nsId, null, DFSUtil.LOCAL_ADDRESS_MATCHER);\n    if (suffixes == null) {\n      String msg = \"Configuration \" + DFS_NAMENODE_RPC_ADDRESS_KEY + \n          \" must be suffixed with nameservice and namenode ID for HA \" +\n          \"configuration.\";\n      throw new HadoopIllegalArgumentException(msg);\n    }\n    \n    return suffixes[1];\n  }",
      "noOfNonPrimitiveParameters": 1,
      "noOfLinesOfCode": 17,
      "difficultyLevel": "medium"
  },
  {
      "functionId": 12,
      "functionQualifiedName": "org.apache.hadoop.hdfs.HAUtil.isHAEnabled",
      "serviceName": "hadoop-hdfs-project/hadoop-hdfs",
      "functionStr": "public static boolean isHAEnabled(Configuration conf, String nsId) {\n    Map<String, Map<String, InetSocketAddress>> addresses =\n        DFSUtilClient.getHaNnRpcAddresses(conf);\n    if (addresses == null) return false;\n    Map<String, InetSocketAddress> nnMap = addresses.get(nsId);\n    return nnMap != null && nnMap.size() > 1;\n  }",
      "noOfNonPrimitiveParameters": 1,
      "noOfLinesOfCode": 7,
      "difficultyLevel": "difficult"
  },
  {
      "functionId": 13,
      "functionQualifiedName": "org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Metrics.addWrite",
      "serviceName": "hadoop-hdfs-project/hadoop-hdfs-nfs",
      "functionStr": "public void addWrite(long latencyNanos) {\n    write.add(latencyNanos);\n    for (MutableQuantiles q : writeNanosQuantiles) {\n      q.add(latencyNanos);\n    }\n  }",
      "noOfNonPrimitiveParameters": 0,
      "noOfLinesOfCode": 6,
      "difficultyLevel": "easy"
  },
  {
      "functionId": 14,
      "functionQualifiedName": "org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Metrics.create",
      "serviceName": "hadoop-hdfs-project/hadoop-hdfs-nfs",
      "functionStr": "public static Nfs3Metrics create(Configuration conf, String gatewayName) {\n    String sessionId = conf.get(DFSConfigKeys.DFS_METRICS_SESSION_ID_KEY);\n    MetricsSystem ms = DefaultMetricsSystem.instance();\n    JvmMetrics jm = JvmMetrics.create(gatewayName, sessionId, ms);\n\n    // Percentile measurement is [50th,75th,90th,95th,99th] currently \n    int[] intervals = conf\n        .getInts(NfsConfigKeys.NFS_METRICS_PERCENTILES_INTERVALS_KEY);\n    return ms.register(new Nfs3Metrics(gatewayName, sessionId, intervals, jm));\n  }",
      "noOfNonPrimitiveParameters": 1,
      "noOfLinesOfCode": 10,
      "difficultyLevel": "medium"
  },
  {
      "functionId": 15,
      "functionQualifiedName": "org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Utils.writeChannel",
      "serviceName": "hadoop-hdfs-project/hadoop-hdfs-nfs",
      "functionStr": "public static void writeChannel(Channel channel, XDR out, int xid) {\n    if (channel == null) {\n      RpcProgramNfs3.LOG\n          .info(\"Null channel should only happen in tests. Do nothing.\");\n      return;\n    }\n    \n    if (RpcProgramNfs3.LOG.isDebugEnabled()) {\n      RpcProgramNfs3.LOG.debug(WRITE_RPC_END + xid);\n    }\n    ByteBuf outBuf = XDR.writeMessageTcp(out, true);\n    channel.writeAndFlush(outBuf);\n  }",
      "noOfNonPrimitiveParameters": 2,
      "noOfLinesOfCode": 13,
      "difficultyLevel": "difficult"
  },
  {
      "functionId": 16,
      "functionQualifiedName": "org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token.toString",
      "serviceName": "hadoop-common-project/hadoop-auth",
      "functionStr": "public String toString() {\n      String value = \"\";\n      HttpCookie authCookie = cookieHandler.getAuthCookie();\n      if (authCookie != null) {\n        value = authCookie.getValue();\n        if (value.startsWith(\"\\\"\")) { // tests don't want the quotes.\n          value = value.substring(1, value.length()-1);\n        }\n      }\n      return value;\n    }",
      "noOfNonPrimitiveParameters": 0,
      "noOfLinesOfCode": 11,
      "difficultyLevel": "easy"
  },
  {
      "functionId": 17,
      "functionQualifiedName": "org.apache.hadoop.security.authentication.util.KerberosUtil.getPrincipalNames",
      "serviceName": "hadoop-common-project/hadoop-auth",
      "functionStr": "public static final String[] getPrincipalNames(String keytab,\n      Pattern pattern) throws IOException {\n    String[] principals = getPrincipalNames(keytab);\n    if (principals.length != 0) {\n      List<String> matchingPrincipals = new ArrayList<String>();\n      for (String principal : principals) {\n        if (pattern.matcher(principal).matches()) {\n          matchingPrincipals.add(principal);\n        }\n      }\n      principals = matchingPrincipals.toArray(new String[0]);\n    }\n    return principals;\n  }",
      "noOfNonPrimitiveParameters": 1,
      "noOfLinesOfCode": 14,
      "difficultyLevel": "medium"
  },
  {
      "functionId": 18,
      "functionQualifiedName": "org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection",
      "serviceName": "hadoop-common-project/hadoop-auth",
      "functionStr": "public HttpURLConnection openConnection(URL url, Token token) throws IOException, AuthenticationException {\n    if (url == null) {\n      throw new IllegalArgumentException(\"url cannot be NULL\");\n    }\n    if (!url.getProtocol().equalsIgnoreCase(\"http\") && !url.getProtocol().equalsIgnoreCase(\"https\")) {\n      throw new IllegalArgumentException(\"url must be for a HTTP or HTTPS resource\");\n    }\n    if (token == null) {\n      throw new IllegalArgumentException(\"token cannot be NULL\");\n    }\n    authenticator.authenticate(url, token);\n\n    // allow the token to create the connection with a cookie handler for\n    // managing session cookies.\n    return token.openConnection(url, connConfigurator);\n  }",
      "noOfNonPrimitiveParameters": 2,
      "noOfLinesOfCode": 21,
      "difficultyLevel": "difficult"
  },
  {
      "functionId": 19,
      "functionQualifiedName": "org.apache.hadoop.registry.client.binding.RegistryTypeUtils.getAddressField",
      "serviceName": "hadoop-common-project/hadoop-registry",
      "functionStr": "public static String getAddressField(Map<String, String> address,\n      String field) throws InvalidRecordException {\n    String val = address.get(field);\n    if (val == null) {\n      throw new InvalidRecordException(\"\", \"Missing address field: \" + field);\n    }\n    return val;\n  }",
      "noOfNonPrimitiveParameters": 0,
      "noOfLinesOfCode": 8,
      "difficultyLevel": "easy"
  },
  {
      "functionId": 20,
      "functionQualifiedName": "org.apache.hadoop.registry.client.binding.RegistryTypeUtils.retrieveAddressesUriType",
      "serviceName": "hadoop-common-project/hadoop-registry",
      "functionStr": "public static List<String> retrieveAddressesUriType(Endpoint epr)\n      throws InvalidRecordException {\n    if (epr == null) {\n      return null;\n    }\n    requireAddressType(ADDRESS_URI, epr);\n    List<Map<String, String>> addresses = epr.addresses;\n    if (addresses.size() < 1) {\n      throw new InvalidRecordException(epr.toString(),\n          \"No addresses in endpoint\");\n    }\n    List<String> results = new ArrayList<String>(addresses.size());\n    for (Map<String, String> address : addresses) {\n      results.add(getAddressField(address, ADDRESS_URI));\n    }\n    return results;\n  }",
      "noOfNonPrimitiveParameters": 1,
      "noOfLinesOfCode": 17,
      "difficultyLevel": "medium"
  },
  {
      "functionId": 21,
      "functionQualifiedName": "org.apache.hadoop.registry.client.binding.RegistryUtils.statChildren",
      "serviceName": "hadoop-common-project/hadoop-registry",
      "functionStr": "public static Map<String, RegistryPathStatus> statChildren(\n      RegistryOperations registryOperations,\n      String path)\n      throws PathNotFoundException,\n      InvalidPathnameException,\n      IOException {\n    List<String> childNames = registryOperations.list(path);\n    Map<String, RegistryPathStatus> results =\n        new HashMap<String, RegistryPathStatus>();\n    for (String childName : childNames) {\n      String child = join(path, childName);\n      try {\n        RegistryPathStatus stat = registryOperations.stat(child);\n        results.put(childName, stat);\n      } catch (PathNotFoundException pnfe) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"stat failed on {}: moved? {}\", child, pnfe, pnfe);\n        }\n        // and continue\n      }\n    }\n    return results;\n  }",
      "noOfNonPrimitiveParameters": 1,
      "noOfLinesOfCode": 23,
      "difficultyLevel": "difficult"
  },
  {
      "functionId": 22,
      "functionQualifiedName": "org.apache.hadoop.crypto.key.kms.server.KMSWebServer.getKMSUrl",
      "serviceName": "hadoop-common-project/hadoop-kms",
      "functionStr": "public URL getKMSUrl() {\n    InetSocketAddress addr = httpServer.getConnectorAddress(0);\n    if (null == addr) {\n      return null;\n    }\n    try {\n      return new URL(scheme, addr.getHostName(), addr.getPort(),\n          SERVLET_PATH);\n    } catch (MalformedURLException ex) {\n      throw new RuntimeException(\"It should never happen: \" + ex.getMessage(),\n          ex);\n    }\n  }",
      "noOfNonPrimitiveParameters": 0,
      "noOfLinesOfCode": 13,
      "difficultyLevel": "easy"
  },
  {
      "functionId": 23,
      "functionQualifiedName": "org.apache.hadoop.crypto.key.kms.server.KMSACLs.isACLPresent",
      "serviceName": "hadoop-common-project/hadoop-kms",
      "functionStr": "public boolean isACLPresent(String keyName, KeyOpType opType) {\n    return (keyAcls.containsKey(keyName)\n        || defaultKeyAcls.containsKey(opType)\n        || whitelistKeyAcls.containsKey(opType));\n  }",
      "noOfNonPrimitiveParameters": 1,
      "noOfLinesOfCode": 5,
      "difficultyLevel": "medium"
  },
  {
      "functionId": 24,
      "functionQualifiedName": "org.apache.hadoop.crypto.key.kms.server.KMSACLs.hasAccessToKey",
      "serviceName": "hadoop-common-project/hadoop-kms",
      "functionStr": "public boolean hasAccessToKey(String keyName, UserGroupInformation ugi,\n      KeyOpType opType) {\n    boolean access = checkKeyAccess(keyName, ugi, opType)\n        || checkKeyAccess(whitelistKeyAcls, ugi, opType);\n    if (!access) {\n      KMSWebApp.getKMSAudit().unauthorized(ugi, opType, keyName);\n    }\n    return access;\n  }",
      "noOfNonPrimitiveParameters": 2,
      "noOfLinesOfCode": 9,
      "difficultyLevel": "difficult"
  },
  {
      "functionId": 25,
      "functionQualifiedName": "org.apache.hadoop.fs.cosn.ByteBufferInputStream.read",
      "serviceName": "hadoop-cloud-storage-project/hadoop-cos",
      "functionStr": "public int read() throws IOException {\n    if (null == this.byteBuffer) {\n      throw new IOException(\"this byte buffer for InputStream is null\");\n    }\n    if (!this.byteBuffer.hasRemaining()) {\n      return -1;\n    }\n    return this.byteBuffer.get() & 0xFF;\n  }",
      "noOfNonPrimitiveParameters": 0,
      "noOfLinesOfCode": 9,
      "difficultyLevel": "easy"
  },
  {
      "functionId": 26,
      "functionQualifiedName": "org.apache.hadoop.fs.cosn.CosNFileSystem.open",
      "serviceName": "hadoop-cloud-storage-project/hadoop-cos",
      "functionStr": "public FSDataInputStream open(Path f, int bufferSize) throws IOException {\n    FileStatus fs = getFileStatus(f); // will throw if the file doesn't\n    // exist\n    if (fs.isDirectory()) {\n      throw new FileNotFoundException(\"'\" + f + \"' is a directory\");\n    }\n    LOG.info(\"Open the file: [{}] for reading.\", f);\n    Path absolutePath = makeAbsolute(f);\n    String key = pathToKey(absolutePath);\n    long fileSize = store.getFileLength(key);\n    return new FSDataInputStream(new BufferedFSInputStream(\n        new CosNInputStream(this.getConf(), store, statistics, key, fileSize,\n            this.boundedIOThreadPool), bufferSize));\n  }",
      "noOfNonPrimitiveParameters": 1,
      "noOfLinesOfCode": 14,
      "difficultyLevel": "medium"
  },
  {
      "functionId": 27,
      "functionQualifiedName": "org.apache.hadoop.fs.cosn.CosNFileSystem.mkdirs",
      "serviceName": "hadoop-cloud-storage-project/hadoop-cos",
      "functionStr": "public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n    try {\n      FileStatus fileStatus = getFileStatus(f);\n      if (fileStatus.isDirectory()) {\n        return true;\n      } else {\n        throw new FileAlreadyExistsException(\"Path is a file: \" + f);\n      }\n    } catch (FileNotFoundException e) {\n      validatePath(f);\n    }\n\n    return mkDirRecursively(f, permission);\n  }",
      "noOfNonPrimitiveParameters": 2,
      "noOfLinesOfCode": 14,
      "difficultyLevel": "difficult"
  },
  {
      "functionId": 28,
      "functionQualifiedName": "org.apache.hadoop.security.authentication.util.KerberosName.getDefaultRealm",
      "serviceName": "hadoop-common-project/hadoop-auth",
      "functionStr": "public static synchronized String getDefaultRealm() {\n    if (defaultRealm == null) {\n      try {\n        defaultRealm = KerberosUtil.getDefaultRealm();\n      } catch (Exception ke) {\n        LOG.debug(\"Kerberos krb5 configuration not found, setting default realm to empty\");\n        defaultRealm = \"\";\n      }\n    }\n    return defaultRealm;\n  }",
      "noOfNonPrimitiveParameters": 0,
      "noOfLinesOfCode": 11,
      "difficultyLevel": "easy"
  },
  {
      "functionId": 29,
      "functionQualifiedName": "org.apache.hadoop.security.authentication.util.KerberosName$Rule.replaceSubstitution",
      "serviceName": "hadoop-common-project/hadoop-auth",
      "functionStr": "static String replaceSubstitution(String base, Pattern from, String to,\n                                      boolean repeat) {\n      Matcher match = from.matcher(base);\n      if (repeat) {\n        return match.replaceAll(to);\n      } else {\n        return match.replaceFirst(to);\n      }\n    }",
      "noOfNonPrimitiveParameters": 1,
      "noOfLinesOfCode": 9,
      "difficultyLevel": "medium"
  },
  {
      "functionId": 30,
      "functionQualifiedName": "org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token.injectToken",
      "serviceName": "hadoop-common-project/hadoop-auth",
      "functionStr": "public static void injectToken(HttpURLConnection conn, Token token) {\n    HttpCookie authCookie = token.cookieHandler.getAuthCookie();\n    if (authCookie != null) {\n      conn.addRequestProperty(\"Cookie\", authCookie.toString());\n    }\n  }",
      "noOfNonPrimitiveParameters": 2,
      "noOfLinesOfCode": 6,
      "difficultyLevel": "difficult"
  }
]
