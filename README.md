# Tool to generate unit tests using LLM and CodeQL

## Project Overview:

---

### Problem Statement:

During the initial exploration, the project identified a challenge in generating accurate unit tests utilizing ChatGPT for Java methods. The complexity of functions and their dependencies on other Java classes resulted in inaccurate unit test generation due to insufficient context within ChatGPT's prompt, limited by its prompt length constraints.

### Proposed Solution:

To overcome the challenge, the project proposes leveraging CodeQL for static analysis of the project. CodeQL's capabilities enable the extraction of metadata about all classes within the codebase, aiding in the identification of classes associated with the target Java function. By amalgamating the metadata from all classes and the list of dependent classes, this approach aims to augment ChatGPT's context. The enriched context, encompassing CodeQL-derived metadata alongside the target function, is anticipated to enhance the accuracy and quality of unit tests generated by ChatGPT.

This project initiative seeks to optimize unit test generation by combining the strengths of CodeQL's static analysis capabilities with ChatGPT's natural language processing prowess, potentially overcoming the limitations observed during the initial phases of testing ChatGPT for unit test generation in Java.

### Project Report:
[PDF](extras/final-report/Unit-Test-Generation-using-LLM-and-CodeQL.pdf)
<br>
[Notion](https://www.notion.so/Unit-Test-Generation-using-LLM-and-CodeQL-3e207041ca234802817e6c175158e566?pvs=4)

## Architecture Diagram:

---

![Architecture Diagram](/extras/images/Architecture-Diagram.png).

The tool mainly consists of the following two modules:

## 1. CodeQL

- Static code analysis using code to get metadata of the project.
- Expected to get variables and functions for each Java class.

## 2. LLM (ChatGPT)

- Input to ChatGPT:
  - Target function
  - Metadata for all classes which are being used in the target function
  - Prompt
- Semantically compare generated unit test with expected unit test

## Setting up

---

### CodeQL

We are using the VS Code extension of the CodeQL to work on this tool. So, ensure that you've VS Code installed in your system.

1. Setting up CodeQL CLI: [CodeQL CLI Setup](https://docs.github.com/en/code-security/codeql-cli/getting-started-with-the-codeql-cli/setting-up-the-codeql-cli)
2. Setting up VS Code extension: [VS Code](https://marketplace.visualstudio.com/items?itemName=github.vscode-codeql)

Verify the setup as mentioned in the CLI Setup link.

### Python script

Note that you need `python` and `pip3` installed before proceeding.

#### Setting up the OpenAI API Key

This would require an OpenAI API key which can be obtained from [OpenAI API Key](https://platform.openai.com/account/api-keys). Save the key in a file named `config.json` in the `/tool` folder in the format given below.

```
{
  "OPENAI_API_KEY": "<KEY>"
}
```

#### Install all the requirements using the `requirements.txt` file

```
pip3 install -r tool/requirements.txt
```

#### Navigate to the `tool/` directory

```
cd tool
```

## Finalized Prompts

---

Here is a template of the prompt we will be using in our tool: [Prompt Template](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/blob/main/data/prompts-template.md). The example following our prompt is [Sample Prompt Example](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/blob/main/data/sample-prompt-example.txt). Here is the example of [ChatGPT Prompt](https://chat.openai.com/share/4b249890-523f-4bf1-924c-9b2a0156bf47).

## Running the Tool

---

### CodeQL

1. Import the Hadoop database in CodeQL through VS Code extension [Hadoop GitHub Home](https://github.com/apache/hadoop)
2. Select the **Java** language while importing the project. (This may take a while)
3. In VS Code, open a command Palette (Ctrl+shift+P) and select **CodeQL: Quick Query**. This will open the `quick-query.ql` file.
4. Copy the query from [Get Class Methods](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/blob/main/codql-scripts/get-methods-of-class.ql) and paste in the `quick-query.ql`
5. Right-Click on an editor and select `CodeQL: Run Query on Selected Database`
6. This will generate an output similar to [Class to Methods mapping](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/blob/main/codeql-query-results/get-methods-of-class.csv)

### Running the script

```
python run_tool.py [--mode] [--n]
```

_Note: The script takes two arguments `--mode` and `--n`._
<br>
`--mode`:

- Indicates in which mode you want to run your script.
- Inputs: 0, 1, 2, 3, 4 (**Default: 4**).
- 0 -> Without Meta-data.
- 1 -> With only method's class meta-data.
- 2 -> With only method_params.
- 3 -> With only method_vars.
- 4 -> With all Meta-data.

`--n`:

- Indicates the number of data points you want to run it on.
- Inputs: Integer between 1 and 30 (**Default: 1**)

## Sample Data Point Execution

---

### Step 1: Setup

- Ensure that you have completed the [setup](#setting-up) process.

### Step 2: Running CodeQL Queries for Meta-Data CSV

- Ensure Hadoop repo is setup.
- Access the provided CodeQL queries in the [codeql-scripts](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/tree/main/codql-scripts) directory.
- Execute these queries to extract meta-data from the Hadoop project and save the result set as CSV files in the [codeql-query-results](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/tree/main/codeql-query-results) directory.
- Ensure the name of the CSV is same as the name of the query file.

```
import java

from
    Class c, ImportType i
where
    i.getFile() = c.getFile()
    and
    c.getFile().getAbsolutePath().matches("%src/main/java%")
select
    c.getQualifiedName() as clsQualifiedName,
    i.getImportedType().toString() as importedCls,
    i.getImportedType().getQualifiedName() as importedClsQualifiedName,
    i.getImportedType().getFile().getRelativePath() as importedClsRelaivePath,
    c.getFile().getRelativePath() as relativePath
```

### Step 3: Convert Meta-Data CSV to JSON

- Utilize the [convert_csv_to_json.py](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/blob/main/tool/helpers/convert_csv_to_json.py) to parse the meta-data CSV files into JSON files (class-metadata.json and method-metadata.json) which will be generated in the [json](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/tree/main/tool/output/json) directory within the `tool/output` folder.
- From the root of repository, change directory to `tool` and then run the convert_csv_to_json script to parse the metadata in CSV to JSON.

```
cd tool
python helpers/convert_csv_to_json.py
```

- After the tool completes execution, the two JSON files: `class-metadata.json` and `method-metadata.json` can be found in [tool/output/json](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/tree/main/tool/output/json) directory.

**\*Note**: We have uploaded a zip of these json files, you can also extract them here and proceed with the next step.\*

### Step 4: Execution to Generate Unit Tests

- From the root of the repo, navigate to the `tool` directory and then execute [run_tool.py](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/blob/main/tool/run_tool.py).
- Set the mode to `4` and the number of data points to `1` (--mode=4 --n=1) to generate a unit test for the method with ID `28` in [methods.json](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/blob/main/data/methods.json).
- Once executed, locate the generated JUnit file in the [generated-tests/all](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/tree/main/tool/output/generated-tests/all) directory.

```
cd tool
python run_tool.py --mode=4 --n=1
```

- These instructions outline the process flow for extracting meta-data, converting it to JSON format, and executing the tool to generate unit tests for specific data points within the project.

- Once the unit tests are generated, they will be stored in the [output/generated-tests](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/tree/main/tool/output/generated-tests) directory. Depending on the execution mode, the generated files will be located in one of the following sub-directories:

  - 0 -> `wo-metadata` sub-directory.
  - 1 -> `class-metadata` sub-directory.
  - 2 -> `method-params` sub-directory.
  - 3 -> `method-vars` sub-directory.
  - 4 -> `all` sub-directory.

- For this example, we have executed the method `getDefaultRealm` and after the `run_tool` script completes it's execution it will generate a JUnit test file at location [output/generated-tests/all/28-1700699735.java](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/blob/main/tool/output/generated-tests/all/28-1700699735.java). Below is the java file generated by our tool:

```java
import org.junit.Test;
import static org.junit.Assert.*;
import org.mockito.Mockito;
import org.apache.hadoop.security.authentication.util.KerberosName;

public class TestDefaultRealm {

@Test
public void testGetDefaultRealm() throws Exception {
// Mock the KerberosName class and its methods
KerberosName kerberosNameMock = Mockito.mock(KerberosName.class);
Mockito.when(kerberosNameMock.getDefaultRealm()).thenReturn("testRealm");

    // Create an instance of the parent class using the mocked KerberosName
    YourParentClass parentObj = new YourParentClass();
    parentObj.kerberosUtil = kerberosNameMock;

    // Call the method under test
    String defaultRealm = parentObj.getDefaultRealm();

    // Verify the result
    assertEquals("testRealm", defaultRealm);

}

// Add more test cases as needed

}
```

- The test wasn't executed directly; we had to modify a few lines of code and comment out some unnecessary lines. The altered java file, shown below, runs successfully. After running the JaCoCo report to obtain code coverage, we achieved a `Statement Coverage of 53%` and `Branch Coverage of 50%`. The adjusted code is as follows

```java
import org.apache.hadoop.security.authentication.util.KerberosUtil;
import org.junit.Test;
import static org.junit.Assert.*;

import org.mockito.MockedStatic;
import org.mockito.Mockito;
import org.apache.hadoop.security.authentication.util.KerberosName;

public class GetDefaultRealmTest {

//    @Mock
//    KerberosName kereberosName;

    @Test
    public void testGetDefaultRealm() throws Exception {
        // Mock the KerberosName class and its methods
//        KerberosUtil kerberosUtilsMock = Mockito.mock(KerberosUtil.class);
        MockedStatic<KerberosUtil> utilities = Mockito.mockStatic(KerberosUtil.class);
        utilities.when(KerberosUtil::getDefaultRealm).thenReturn("testRealm");
//        Mockito.when(utilities.getDefaultRealm()).thenReturn("testRealm");

        // Create an instance of the parent class using the mocked KerberosName
//        KerberosName parentObj = new KerberosName();
//        parentObj.kerberosUtil = kerberosNameMock;

        // Call the method under test
        String defaultRealm = KerberosName.getDefaultRealm();

        // Verify the result
        assertEquals("testRealm", defaultRealm);
    }

    // Add more test cases as needed

}
```

### Here is an image showing `getDefaultRealm` method's code coverage:

![28-jacoco](/extras/images/GetDefaultRealmTest-JaCoCo.jpeg)

This is a complete end-to-end flow of our tool and other results found on [output/generated-tests](https://github.com/ChinuSaraf/unit-test-generation-using-llm-and-codeql/tree/main/tool/output/generated-tests) directory are run using different `mode` and `n` arguments passed to the tool.

---
